---
title: Nightingale-Categraf采集器
date: 2023-05-19 04:19:39
permalink: /pages/9372a5/
categories:
  - 运维
  - Nightingale
tags:
  - 
author: 
  name: zhang
  link: https://github.com/zhangst94
---
# Nightingale-Categraf采集器

​        Categraf 是一个监控采集 Agent，类似  Telegraf、Grafana-Agent、Datadog-Agent，希望对所有常见监控对象提供监控数据采集能力，采用 All-in-one 的设计，不但支持指标采集，也希望支持日志和调用链路的数据采集。

**categraf 和 telegraf、exporters、grafana-agent、datadog-agent 对比**

- 支持 remote_write 写入协议，支持将数据写入 promethues、M3DB、VictoriaMetrics、InfluxDB
- 指标数据只采集数值，不采集字符串，标签维持稳态结构
- 采用 all-in-one 的设计，所有的采集工作用一个 agent 搞定，未来也可以把日志和 trace 的采集纳入 agent
- 纯 Go 代码编写，静态编译依赖少，容易分发，易于安装
- 尽可能落地最佳实践，不需要采集的数据无需采集，针对可能会对时序库造成高基数的问题在采集侧做出处理
- 常用的采集器，不但提供采集能力，还要整理出监控大盘和告警规则，用户可以直接导入使用



Categraf下载地址：https://flashcat.cloud/download/categraf/

## Categraf 配置详解

Categraf 配置文件在conf 目录，conf 下有一个主配置文件：config.toml，定义机器名、全局采集频率、全局附加标签、remote write  backend地址等；另外就是各种采集插件的配置目录，以input.打头，如果某个采集器 xx 不想启用，把 input.xx  改个其他前缀(或者删除这个目录)，比如 bak.input.xx，categraf 就会忽略这个采集器。

categraf启动时 可以通过`-configs`参数指定配置目录，如果不指定，会默认读取工作目录下的conf。 conf 目录结构如下：

- `config.toml` # 主配置
- `logs.toml` # logs-agent 配置
- `prometheus.toml` # prometheus agent 配置
- `traces.yaml` # trace-agent 配置
- `conf/input.*/*.toml` 插件配置文件

## config.toml配置

```toml
[global]
# 启动的时候是否在stdout中打印配置内容
print_configs = false

# 机器名，作为本机的唯一标识，会为时序数据自动附加一个 agent_hostname=$hostname 的标签
# hostname 配置如果为空，自动取本机的机器名
# hostname 配置如果不为空，就使用用户配置的内容作为hostname
# 用户配置的hostname字符串中，可以包含变量，目前支持两个变量，
# $hostname 和 $ip，如果字符串中出现这两个变量，就会自动替换
# $hostname 自动替换为本机机器名，$ip 自动替换为本机IP
# 建议大家使用 --test 做一下测试，看看输出的内容是否符合预期
# 这里配置的内容，再--test模式下，会显示为 agent_hostname=xxx 的标签 
hostname = ""


# 是否忽略主机名的标签，如果设置为true，时序数据中就不会自动附加agent_hostname=$hostname 的标签
omit_hostname = false

# 时序数据的时间戳使用ms还是s，默认是ms，是因为remote write协议使用ms作为时间戳的单位
precision = "ms"

# 全局采集频率，15秒采集一次   ## 配置为数字，单位是秒，配置成字符串，就要给出单位："60s"、"1m"、“1h”
interval = 15

# 全局附加标签，一行一个，这些写的标签会自动附到时序数据上
# [global.labels]
# region = "shanghai"
# env = "localhost"

[log]
# 默认的log输出，到标准输出(stdout) 
# 如果指定为文件, 则写入到指定的文件中
file_name = "stdout"

# options below will not be work when file_name is stdout or stderr
# 如果是写入文件，最大写入大小，单位是MB
max_size = 100
# max_age is the maximum number of days to retain old log files based on the timestamp encoded in their filename.
# 保留多少天的日志文件
max_age = 1
# max_backups is the maximum number of old log files to retain.
# 保留多少个日志文件
max_backups = 1
# local_time determines if the time used for formatting the timestamps in backup files is the computer's local time.
# 是否使用本地时间
local_time = true
# Compress determines if the rotated log files should be compressed using gzip.
# 是否将老文件压缩（gzip格式)
compress = false

# 发给后端的时序数据，会先被扔到 categraf 内存队列里，每个采集插件一个队列
# chan_size 定义了队列最大长度
# batch 是每次从队列中取多少条，发送给后端backend
[writer_opt]
# default: 2000
batch = 2000
# channel(as queue) size
chan_size = 10000

# 后端backend配置，在toml中 [[]] 表示数组，所以可以配置多个writer
# 每个writer可以有不同的url，不同的basic auth信息
[[writers]]
# 注意端口号
# v5版本端口是19000
# v6版本端口是17000
url = "http://127.0.0.1:19000/prometheus/v1/write"

# Basic auth username
basic_auth_user = ""

# Basic auth password
basic_auth_pass = ""

# timeout settings, unit: ms
timeout = 5000
dial_timeout = 2500
max_idle_conns_per_host = 100

# 是否开启push gateway
[http]
enable = false
address = ":9100"
print_access = false
run_mode = "release"

# 是否启用告警自愈agent
[ibex]
enable = false
## ibex flush interval
interval = "1000ms"
## n9e ibex server rpc address
servers = ["127.0.0.1:20090"]
## temp script dir
meta_dir = "./meta"

# 心跳上报（附带资源信息,对象列表中使用）给夜莺v6
# 如果是v5版本，这里不需要保留
[heartbeat]
enable = true

# report os version cpu.util mem.util metadata
url = "http://127.0.0.1:17000/v1/n9e/heartbeat"

# interval, unit: s
interval = 10

# Basic auth username
basic_auth_user = ""

# Basic auth password
basic_auth_pass = ""

## Optional headers
# headers = ["X-From", "categraf", "X-Xyz", "abc"]

# timeout settings, unit: ms
timeout = 5000
dial_timeout = 2500
max_idle_conns_per_host = 100
```

## conf/logs.toml 配置

```toml
[logs]
# api_key http模式下生效,用于鉴权, 其他模式下占位符
api_key = "TCXDuFMmYKPBaruXzWeDwyKFpk4gxslz"

# 是否开启log-agent
enable = true

# 日志发送到哪  send_to 表示推送的目标地址。 ## 当send_type 为kafka类型时，send_to地址支持逗号分割的broker地址列表，topic选项为指定的kafka topic。
send_to = "47.111.118.27:9090" 

# 接受日志的后端类型,tcp,http,kafka  ## send_type 表示推送目标地址的类型，支持kafka/tcp/http，
send_type = "http"

# 日志对应的topic ,kafka模式下生效
topic = "test_log_topic" 

# 是否压缩  ## use_compress 表示压缩发送，http下支持gzip压缩 tcp协议支持zlib压缩
use_compress = false 

# 是否采用tls发送
send_with_tls = false 

# 批量发送的等待时间
batch_wait = 5 

# categraf 记录日志偏移的目录，需要有写权限
run_path = "xxxx" 

# 最大打开的文件数   ## categraf单个进程 最大同时打开的日志限制，默认100个文件
open_files_limit = 100
# 扫描目录的周期
scan_period = 10  
# udp采集的帧大小
frame_size = 10

# 是否采集pod的stdout/stderr日志
collect_container_all = false 

# 只采集哪些pod的stdout/stderr
   ## name:log-demo 表示采集container name匹配log-demo字符的pod    正则匹配
   ## image:xxx 表示采集image 匹配 xxx的pod
   ## namespace:xxxx 表示采集namespace 匹配xxx下的pod
container_include=[""] 
# 排除哪些pod的stdout/stderr  ## 如果只采集某些pod的日志，需要将container_exclude=["name:.*"] 同时配置上。
container_exclude=[""] 

  # 全局处理规则, 该处不支持多行合并。多行日志合并需要在logs.items中配置
  [[logs.processing_rules]] 
   type = "exclude_at_match"
   name = "exclude_xxx_users"
   pattern="\\w+@flashcat.cloud" 
  
  # 日志采集配置
  [[logs.items]]
  # 日志类型 file  ## 采集的类型，当前主要支持file 和 journald。
  type = "file"
  # 日志路径，支持统配符，用统配符，默认从最新位置开始采集
  path="xxxx" 
  # 日志的label 标识日志来源的模块
  source = "tomcat" 
  # 日志的label 标识日志来源的服务
  service = "my_service" 
  # 其他额外想加的tag 
  tags = ["monitoring=test", "k=v"] 
  # kafka 模式下,给单个日志处理项配置不同于全局的topic
  # 如果不配置，则使用全局topic 
  topic = "xxxx"  

  # 日志处理规则
  [[logs.items.log_processing_rules]]
   ## 表示日志中匹配到@flashcat.cloud 的行 不发送 
   type = "exclude_at_match"
   name = "exclude_xxx_users"
   pattern="\\w+@flashcat.cloud"
   ## 表示日志中匹配到2022开头的行 才发送 
   type = "include_at_match"
   name = "include_demo"
   pattern="^2022*"
   ## 表示186的手机号会被[186xxx] 代替
   type = "mask_sequences"
   name = "mask_phone_number"
   replace_placeholder = "[186xxx]"
   pattern="186\\d{8}"
   ## 表示以日期为日志的开头，多行的日志合并为一行进行采集  multi_line不支持全局处理规则
   type = "multi_line" 
   name = "new_line_with_date"
   pattern="\\d{4}-\\d{2}-\\d{2}" （多行规则不需要添加^ ，代码会自动添加）
```

## prometheus.toml 配置

`prometheus-agent`是为了兼容`prometheus agent mode`，所以我们也把它叫做`prometheus-agent`。它可以把`prometheus`采集配置直接拿来使用。

```toml
[prometheus]
# 是否启动prometheus agent
enable=false
# 原来prometheus的配置文件
# 或者新建一个prometheus格式的配置文件
scrape_config_file="/path/to/in_cluster_scrape.yaml"
## 日志级别，支持 debug | warn | info | error
log_level="info"
# 以下配置文件，保持默认就好了
## wal file storage path ,default ./data-agent
# wal_storage_path="/path/to/storage"
## wal reserve time duration, default value is 2 hour
# wal_min_duration=2
```

Prometheus 自动采集 kube-state-metrics 指标的 scrape 配置：

```yaml
global:
  scrape_interval: 15s
  external_labels:
    scraper: ksm-test
    cluster: test
scrape_configs:
  - job_name: "kube-state-metrics"
    metrics_path: "/metrics"
    kubernetes_sd_configs:
      - role: endpoints
        api_server: "https://172.31.0.1:443"
        tls_config:
          ca_file: /etc/kubernetes/pki/ca.crt
          cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt
          key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key
          insecure_skip_verify: true
    scheme: http
    relabel_configs:
      - source_labels:
          [
            __meta_kubernetes_namespace,
            __meta_kubernetes_service_name,
            __meta_kubernetes_endpoint_port_name,
          ]
        action: keep
        regex: kube-system;kube-state-metrics;http-metrics


remote_write:
  - url: 'http://172.31.62.213/prometheus/v1/write'
```

## `conf/input.*/*.toml`配置

https://github.com/flashcatcloud/categraf/tree/main/inputs

inputs目录下对应插件的目录里面有 每个插件的使用方式，提供了 README 和默认配置，

`input.exec/exec.toml`

自定义实现指定业务的监控
监控脚本采集到监控数据之后通过相应的格式输出到stdout，categraf截获stdout内容，解析之后传给服务端，

```toml
# # collect interval
# interval = 15

[[instances]]
# # commands, support glob
commands = [
     "/opt/categraf/scripts/*/*.sh"   ## 监控脚本所在位置
]

# # timeout for each command to complete
# timeout = 5

# # interval = global.interval * interval_times
# interval_times = 1

# # mesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3
data_format = "influx"  # 脚本的输出格式支持3种：influx、falcon、prometheus
```

查看采集器输出的监控指标，配置好 `conf/input.exec/exec.toml` ，可以执行命令：`./categraf --test --inputs exec`

